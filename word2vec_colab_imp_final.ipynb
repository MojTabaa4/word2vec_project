{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec implementation skip-gram with keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install hazm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from hazm import word_tokenize, Lemmatizer, Stemmer, Normalizer\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SgmUyOJqpjl0"
   },
   "id": "SgmUyOJqpjl0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "UdcshJKWpjl2"
   },
   "id": "UdcshJKWpjl2"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# InteractiveShell.ast_node_interactivity = \"last_expr\""
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "RZZpzT2Tpjl3"
   },
   "id": "RZZpzT2Tpjl3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import dataset and persian stop words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "with open(\"Shams_Corpus_Paper3.txt\", \"r\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "with open(\"persian_stopw.txt\", \"r\") as file:\n",
    "    raw_stop_words = file.read()\n",
    "\n",
    "stop_words = word_tokenize(raw_stop_words)\n",
    "\n",
    "\n",
    "def remove_persian_stopword(tokens):\n",
    "    # return [word for word in tokens if not word in stop_words and word and word not in proned]\n",
    "    return [word for word in tokens if not word in stop_words and word]"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hFrB4avApjl4"
   },
   "id": "hFrB4avApjl4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing Part\n",
    "### Create Lemmatizer and Stemmer functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    return normalizer.normalize(text)\n",
    "\n",
    "\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "\n",
    "def lemma_tokenizer(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # return [lemmatizer.lemmatize(token).split(\"#\")[0] for token in tokens]\n",
    "\n",
    "\n",
    "stemmer = Stemmer()\n",
    "\n",
    "\n",
    "def stem_tokenizer(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "nNscK8Oupjl5"
   },
   "id": "nNscK8Oupjl5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def text_normalization(text):\n",
    "    raw_text = re.sub(r\"-+|\\d+|\\s+\", \" \", text)\n",
    "    raw_text = normalize_text(raw_text)\n",
    "\n",
    "    return raw_text\n",
    "\n",
    "\n",
    "def tokenize_text(text, type=\"lemma\"):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = remove_persian_stopword(tokens)\n",
    "    if type == \"lemma\":\n",
    "        tokens = remove_persian_stopword(lemma_tokenizer(tokens))\n",
    "    elif type == \"stem\":\n",
    "        tokens = remove_persian_stopword(stem_tokenizer(tokens))\n",
    "\n",
    "    return tokens"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "9psrEsscpjl6"
   },
   "id": "9psrEsscpjl6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word tokenize:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   sentence  \\\n0                                 دیوان شمس تبریزی (غزلیات)   \n1                                               1001 - 1500   \n2  --------------------------------------------------------   \n3                                                      1001   \n4      آه در آن شمع منور چه بود\\tکآتش زد در دل و دل را ربود   \n5         ای زده اندر دل من آتشی\\tسوختم ای دوست بیا زود زود   \n6         صورت دل صورت مخلوق نیست\\tکز رخ دل حسن خدا رو نمود   \n7       جز شکرش نیست مرا چاره ای\\tجز لب او نیست مرا هیچ سود   \n8    یاد کن آن را که یکی صبحدم\\tاین دلم از زلف تو بندی گشود   \n9     جان من اول که بدیدم تو را\\tجان من از جان تو چیزی شنود   \n\n                                         normalized_sent  \\\n0                              دیوان شمس تبریزی (غزلیات)   \n1                                                          \n2                                                          \n3                                                          \n4    آه در آن شمع منور چه بود کآتش زد در دل و دل را ربود   \n5       ای زده اندر دل من آتشی سوختم ای دوست بیا زود زود   \n6       صورت دل صورت مخلوق نیست کز رخ دل حسن خدا رو نمود   \n7     جز شکرش نیست مرا چاره‌ای جز لب او نیست مرا هیچ سود   \n8  یاد کن آن را که یکی صبحدم این دلم از زلف تو بندی گشود   \n9   جان من اول که بدیدم تو را جان من از جان تو چیزی شنود   \n\n                                       tokens  \n0                [دیوان, شمس, تبریزی, غزلیات]  \n1                                          []  \n2                                          []  \n3                                          []  \n4  [آه, شمع, منور, کآتش, زد#زن, دل, دل, ربود]  \n5    [زده, دل, آتش, سوخت#سوز, دوست, زود, زود]  \n6     [دل, مخلوق, رخ, دل, حسن, خدا, رو, نمود]  \n7                        [شکر, چاره, لب, سود]  \n8    [یاد, صبحدم, دل, زلف, بست#بند, گشود#گشا]  \n9                [جان, بدیدم, جان, جان, شنود]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>normalized_sent</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>دیوان شمس تبریزی (غزلیات)</td>\n      <td>دیوان شمس تبریزی (غزلیات)</td>\n      <td>[دیوان, شمس, تبریزی, غزلیات]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1001 - 1500</td>\n      <td></td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>--------------------------------------------------------</td>\n      <td></td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1001</td>\n      <td></td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>آه در آن شمع منور چه بود\\tکآتش زد در دل و دل را ربود</td>\n      <td>آه در آن شمع منور چه بود کآتش زد در دل و دل را ربود</td>\n      <td>[آه, شمع, منور, کآتش, زد#زن, دل, دل, ربود]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ای زده اندر دل من آتشی\\tسوختم ای دوست بیا زود زود</td>\n      <td>ای زده اندر دل من آتشی سوختم ای دوست بیا زود زود</td>\n      <td>[زده, دل, آتش, سوخت#سوز, دوست, زود, زود]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>صورت دل صورت مخلوق نیست\\tکز رخ دل حسن خدا رو نمود</td>\n      <td>صورت دل صورت مخلوق نیست کز رخ دل حسن خدا رو نمود</td>\n      <td>[دل, مخلوق, رخ, دل, حسن, خدا, رو, نمود]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>جز شکرش نیست مرا چاره ای\\tجز لب او نیست مرا هیچ سود</td>\n      <td>جز شکرش نیست مرا چاره‌ای جز لب او نیست مرا هیچ سود</td>\n      <td>[شکر, چاره, لب, سود]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>یاد کن آن را که یکی صبحدم\\tاین دلم از زلف تو بندی گشود</td>\n      <td>یاد کن آن را که یکی صبحدم این دلم از زلف تو بندی گشود</td>\n      <td>[یاد, صبحدم, دل, زلف, بست#بند, گشود#گشا]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>جان من اول که بدیدم تو را\\tجان من از جان تو چیزی شنود</td>\n      <td>جان من اول که بدیدم تو را جان من از جان تو چیزی شنود</td>\n      <td>[جان, بدیدم, جان, جان, شنود]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Shams_Corpus_Paper3.txt\", names=[\"sentence\"])\n",
    "data[\"normalized_sent\"] = data[\"sentence\"].apply(lambda x: text_normalization(x))\n",
    "data[\"tokens\"] = data[\"normalized_sent\"].apply(lambda x: tokenize_text(x))\n",
    "data.head(10)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lWeRDkcVpjl7"
   },
   "id": "lWeRDkcVpjl7"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_df = data[\"tokens\"]\n",
    "del data\n",
    "tokens = token_df.explode().dropna().tolist()\n",
    "len(tokens)\n",
    "tokens[:10]\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "U34cEN9dpjl8"
   },
   "id": "U34cEN9dpjl8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save tokens with pickle serializer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tokens_df_moreth2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(token_df, f)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "MDq2nP1mpjl9"
   },
   "id": "MDq2nP1mpjl9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def concat(*iterables):\n",
    "    for iterable in iterables:\n",
    "        yield from iterable"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "kU1FF5utpjl-"
   },
   "id": "kU1FF5utpjl-"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def generate_training_data(tokens, word_to_id, window):\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     n_tokens = len(tokens)\n",
    "#     unique_tokens = len(word_to_id)\n",
    "#     for i in range(n_tokens):\n",
    "#         idx = concat(\n",
    "#             range(max(0, i - window), i), range(i, min(n_tokens, i + window + 1))\n",
    "#         )\n",
    "#         for j in idx:\n",
    "#             if i == j:\n",
    "#                 continue\n",
    "#             X.append(word_to_id[tokens[i]] - 1)\n",
    "#             y.append(word_to_id[tokens[j]] - 1)\n",
    "\n",
    "#     return np.asarray(X), np.asarray(y)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "7v_7yt9opjl_"
   },
   "id": "7v_7yt9opjl_"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_data(series, word_to_id, window):\n",
    "    X = []\n",
    "    y = []\n",
    "    for index, tokens in series.items():\n",
    "        n_tokens = len(tokens)\n",
    "        for i in range(n_tokens):\n",
    "            idx = concat(\n",
    "                range(max(0, i - window), i),\n",
    "                range(i, min(n_tokens, i + window + 1))\n",
    "            )\n",
    "            for j in idx:\n",
    "                if i == j:\n",
    "                    continue\n",
    "\n",
    "                X.append(word_to_id[tokens[i]] - 1)\n",
    "                y.append(word_to_id[tokens[j]] - 1)\n",
    "\n",
    "    return np.asarray(X), np.asarray(y)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "a4X7JnqapjmA"
   },
   "id": "a4X7JnqapjmA"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### generate training data with specified window size\n",
    "#### create word to id and id to word list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "\n",
    "t = Tokenizer(filters=\"\")\n",
    "t.fit_on_texts(tokens)\n",
    "\n",
    "sorted_count_list = sorted(t.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "word_to_id, id_to_word = t.word_index, t.index_word\n",
    "\n",
    "# X, y = generate_training_data(tokens, word_to_id, window_size)\n",
    "X_sen, y_sen = generate_data(token_df, word_to_id, window_size)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "X-vDeQJjpjmB"
   },
   "id": "X-vDeQJjpjmB"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### delete useless variables cause of lack memory :(("
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# del X_onehot_encoded\n",
    "# del y_onehot_encoded\n",
    "X_sen.shape\n",
    "y_sen.shape\n",
    "del t\n",
    "del X_onehot_encoded\n",
    "del y_onehot_encoded"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Zdb-9QFypjmB"
   },
   "id": "Zdb-9QFypjmB"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for i in sorted_count_list:\n",
    "#     if '#' in i[0]:\n",
    "#         print(i)\n",
    "# sorted_count_list[:40]"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NcnNnNXhpjmC"
   },
   "id": "NcnNnNXhpjmC"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Find less frequent words in corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "proned = []\n",
    "for i in sorted_count_list:\n",
    "    if i[1] < 3:\n",
    "        proned.append(i[0])\n",
    "stop_words.extend(proned)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "txIAZzUhpjmD"
   },
   "id": "txIAZzUhpjmD"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X.shape\n",
    "X_sen.shape"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "RWwc42glpjmD"
   },
   "id": "RWwc42glpjmD"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Onehot train and test tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# echo 1 > /proc/sys/vm/overcommit_memory\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(X_sen)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "one_hotter = onehot_encoder.fit(integer_encoded)\n",
    "X_onehot_encoded = one_hotter.transform(integer_encoded)\n",
    "del X_sen\n",
    "\n",
    "integer_encoded = label_encoder.fit_transform(y_sen)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y_onehot_encoded = one_hotter.transform(integer_encoded)\n",
    "del y_sen"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "yiFaTi0BpjmD"
   },
   "id": "yiFaTi0BpjmD"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"one_hot_encoder.pkl\", \"wb\") as f: \n",
    "#     pickle.dump(one_hotter, f)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "J8-Yeel_pjmE"
   },
   "id": "J8-Yeel_pjmE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_onehot_encoded.shape"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "KurddO8opjmE"
   },
   "id": "KurddO8opjmE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_onehot_encoded.shape"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "WCciI_cjpjmF"
   },
   "id": "WCciI_cjpjmF"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del integer_encoded"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "UjCJmNNCpjmF"
   },
   "id": "UjCJmNNCpjmF"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create network model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Reshape, Embedding\n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense\n",
    "\n",
    "opt = 'adam'\n",
    "embed_size = 100\n",
    "vocab_size = X_onehot_encoded.shape[1]\n",
    "\n",
    "input_layer = Input(shape=(vocab_size,))\n",
    "embed_layer = Dense(units=embed_size, activation=\"linear\")(input_layer)\n",
    "output_layer = Dense(units=vocab_size, activation=\"softmax\")(embed_layer)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=['accuracy', 'mse'])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "riO6bAwEpjmG"
   },
   "id": "riO6bAwEpjmG"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learn network :"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs_ = 100\n",
    "model.fit(x=X_onehot_encoded, y=y_onehot_encoded, batch_size=128, epochs=epochs_, verbose=1)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "aEhCk3p0pjmG"
   },
   "id": "aEhCk3p0pjmG"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def n_similar(word, model_, n=10):\n",
    "    word_id = word_to_id[word]\n",
    "    one_hot = one_hotter.transform(np.array([[word_id]]))\n",
    "    result = model_.predict([one_hot]).squeeze()\n",
    "    for word in (id_to_word[id + 1] for id in np.argsort(result)[::-1][:n]):\n",
    "        print(word)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "UK_inNUtpjmH"
   },
   "id": "UK_inNUtpjmH"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_similar('جهان', model, n=20)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "b1gd4HanpjmH"
   },
   "id": "b1gd4HanpjmH"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save(f'mdl_em{embed_size}_ep{epochs_}_vocs{vocab_size}_ws{window_size}_opt{opt}.h5')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "heWoDPS1pjmI"
   },
   "id": "heWoDPS1pjmI"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create word2vec model with pure python:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_network(vocab_size, n_embedding):\n",
    "    model = {\n",
    "        \"w1\": np.random.uniform(-1, 1, (vocab_size, n_embedding)),\n",
    "        \"w2\": np.random.uniform(-1, 1, (n_embedding, vocab_size))\n",
    "    }\n",
    "    return model"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "MKjD_vNgpjmI"
   },
   "id": "MKjD_vNgpjmI"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = init_network(len(word_to_id), 10)\n",
    "model[\"w1\"].shape"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "qqllL3G_pjmJ"
   },
   "id": "qqllL3G_pjmJ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model[\"w2\"].shape"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "3QA19E1FpjmJ"
   },
   "id": "3QA19E1FpjmJ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    #     e_x = np.exp(x - np.max(x))\n",
    "    #     return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    res = []\n",
    "    for x in X:\n",
    "        exp = np.exp(x)\n",
    "        res.append(exp / exp.sum())\n",
    "    return res"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "8r39d3VwpjmJ"
   },
   "id": "8r39d3VwpjmJ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def stable_sigmoid(x):\n",
    "    sig = np.where(x < 0, np.exp(x) / (1 + np.exp(x)), 1 / (1 + np.exp(-x)))\n",
    "    return sig"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NcPjJCyIpjmK"
   },
   "id": "NcPjJCyIpjmK"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "hp-L7i0BpjmK"
   },
   "id": "hp-L7i0BpjmK"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(X @ model[\"w1\"]).shape"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "CjpjqrzxpjmK"
   },
   "id": "CjpjqrzxpjmK"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(X @ model[\"w1\"] @ model[\"w2\"]).shape\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "PXHVK5otpjmK"
   },
   "id": "PXHVK5otpjmK"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def forward(model, X, return_cache=True):\n",
    "    cache = {}\n",
    "\n",
    "    cache[\"a1\"] = X @ model[\"w1\"]\n",
    "    cache[\"a2\"] = cache[\"a1\"] @ model[\"w2\"]\n",
    "    print(f\"a2 = {cache['a2']}\")\n",
    "    cache[\"z\"] = softmax(cache[\"a2\"])\n",
    "    #     cache[\"z\"] = stable_sigmoid(cache[\"a2\"])\n",
    "\n",
    "    if not return_cache:\n",
    "        return cache[\"z\"]\n",
    "    return cache"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "59PBfArmpjmL"
   },
   "id": "59PBfArmpjmL"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cross_entropy(z, y):\n",
    "    return - np.sum(np.log(z) * y)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6QBTRWFLpjmL"
   },
   "id": "6QBTRWFLpjmL"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def backward(model, X, y, alpha):\n",
    "    cache = forward(model, X)\n",
    "    #     dl_weight_inp_hidden = np.outer(target_word_vector, np.dot(weight_hidden_output, total_error.T))\n",
    "    #     dl_weight_hidden_output = np.outer(hidden_layer, total_error)\n",
    "    da2 = cache[\"z\"] - y\n",
    "    dw2 = cache[\"a1\"].T @ da2\n",
    "    da1 = da2 @ model[\"w2\"].T\n",
    "    dw1 = X.T @ da1\n",
    "    assert (dw2.shape == model[\"w2\"].shape)\n",
    "    assert (dw1.shape == model[\"w1\"].shape)\n",
    "    model[\"w1\"] -= alpha * dw1\n",
    "    model[\"w2\"] -= alpha * dw2\n",
    "\n",
    "    return cross_entropy(cache[\"z\"], y)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "3eb2fzmQpjmL"
   },
   "id": "3eb2fzmQpjmL"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "% config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "model = init_network(len(word_to_id), 10)\n",
    "\n",
    "n_iter = 100\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "history = [backward(model, X, y, learning_rate) for _ in range(n_iter)]\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(range(len(history)), history, color=\"skyblue\")\n",
    "plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "algo-gL0pjmM"
   },
   "id": "algo-gL0pjmM"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "oD2SjzlUpjmM"
   },
   "id": "oD2SjzlUpjmM"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "rLIGbEerpjmM"
   },
   "id": "rLIGbEerpjmM"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning = one_hot_encode(word_to_id[\"گرم\"], len(word_to_id))\n",
    "result = forward(model, [learning], return_cache=False)[0]\n",
    "result"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ApBljDqmpjmN"
   },
   "id": "ApBljDqmpjmN"
  },
  {
   "cell_type": "raw",
   "source": [
    "np.argsort(result)[::-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    },
    "id": "K9Yztq6SpjmN"
   },
   "id": "K9Yztq6SpjmN"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.argsort(result)[::-1][0:5]"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Q29aqvWnpjmO"
   },
   "id": "Q29aqvWnpjmO"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for word in (id_to_word[id] for id in np.argsort(result)[::-1][0:10]):\n",
    "    print(word)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "-4xt4X_ypjmO"
   },
   "id": "-4xt4X_ypjmO"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_word_similarities(word, model, n_similars=10):\n",
    "    try:\n",
    "        learning = one_hot_encode(word_to_id[word] - 1, len(word_to_id))\n",
    "    except KeyError:\n",
    "        print(f\"Word = {word} is not in corpus\")\n",
    "        exit()\n",
    "    result = forward(model, [learning], return_cache=False)[0]\n",
    "    for word in (id_to_word[id + 1] for id in np.argsort(result)[::-1][0:n_similars]):\n",
    "        print(word)\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "5ltvFUr4pjmP"
   },
   "id": "5ltvFUr4pjmP"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_embedding(model, word):\n",
    "    try:\n",
    "        idx = word_to_id[word] - 1\n",
    "    except KeyError:\n",
    "        print(\"`word` not in corpus\")\n",
    "    one_hot = one_hot_encode(idx, len(word_to_id))\n",
    "    return forward(model, one_hot)[\"a1\"]"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "CBlu5oIFpjmP"
   },
   "id": "CBlu5oIFpjmP"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_embedding(model, \"دیو\")\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "e3s7PjVGpjmQ"
   },
   "id": "e3s7PjVGpjmQ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_word_similarities('عیش', model, 10)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "jz83FdBkpjmQ"
   },
   "id": "jz83FdBkpjmQ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_word_similarities('میخانه', model, 10)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "TvP65sDUpjmQ"
   },
   "id": "TvP65sDUpjmQ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_word_similarities('بشر', model, 10)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "53JPB5eQpjmQ"
   },
   "id": "53JPB5eQpjmQ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_word_similarities('ویرانه', model, 10)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "E9LKYt27pjmR"
   },
   "id": "E9LKYt27pjmR"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_word_similarities('حلال', model, 10)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Glsp9R9kpjmR"
   },
   "id": "Glsp9R9kpjmR"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "colab": {
   "name": "word2vec_colab_imp_final.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}