{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Create word2vec model with negative sampling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Install packages and adjust setting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install hazm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from hazm import word_tokenize, Lemmatizer, Stemmer, Normalizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import layers\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# InteractiveShell.ast_node_interactivity = \"last_expr\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import corpus and persian stop words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"data.txt\", \"r\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "with open(\"persian_stopw.txt\", \"r\") as file:\n",
    "    raw_stop_words = file.read()\n",
    "\n",
    "stop_words = word_tokenize(raw_stop_words)\n",
    "\n",
    "\n",
    "def remove_persian_stopword(tokens):\n",
    "    # return [word for word in tokens if not word in stop_words and word and word not in proned]\n",
    "    return [word for word in tokens if not word in stop_words and word]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "stemmer = Stemmer()\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    return normalizer.normalize(text)\n",
    "\n",
    "\n",
    "def lemma_tokenizer(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # return [lemmatizer.lemmatize(token).split(\"#\")[0] for token in tokens]\n",
    "\n",
    "\n",
    "def stem_tokenizer(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def text_normalization(text):\n",
    "    raw_text = re.sub(r\"-+|\\d+|\\s+\", \" \", text)\n",
    "    raw_text = normalize_text(raw_text)\n",
    "\n",
    "    return raw_text\n",
    "\n",
    "\n",
    "def tokenize_text(text, type=\"lemma\"):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = remove_persian_stopword(tokens)\n",
    "    if type == \"lemma\":\n",
    "        tokens = remove_persian_stopword(lemma_tokenizer(tokens))\n",
    "    elif type == \"stem\":\n",
    "        tokens = remove_persian_stopword(stem_tokenizer(tokens))\n",
    "\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create tokens dataframe (normalized + lemmatized + removed persian stop words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.txt\", names=[\"sentence\"])\n",
    "data[\"normalized_sent\"] = data[\"sentence\"].apply(lambda x: text_normalization(x))\n",
    "data[\"tokens\"] = data[\"normalized_sent\"].apply(lambda x: tokenize_text(x))\n",
    "data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_df = data[\"tokens\"]\n",
    "# del data\n",
    "tokens = token_df.explode().dropna().tolist()\n",
    "len(tokens)\n",
    "tokens[:10]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create word to id and id to word with keras tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = Tokenizer(filters=\"\")\n",
    "t.fit_on_texts(tokens)\n",
    "\n",
    "sorted_count_list = sorted(t.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "word_to_id, id_to_word = t.word_index, t.index_word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(word_to_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating training data with number of negative samples and window size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_training_data(sentences, window_size, num_negative_s, vocab_size):\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    centers, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for vocab_size tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all sentences in corpus\n",
    "    for sequence in tqdm_notebook(sentences, desc='Sentenses', colour=\"MAGENTA\"):\n",
    "\n",
    "        # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_samples, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size=vocab_size,\n",
    "            sampling_table=sampling_table,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0\n",
    "        )\n",
    "\n",
    "        # Iterate over each positive skip-gram pair to produce training examples\n",
    "        # with positive context word and negative samples.\n",
    "        for center_word, context_word in positive_samples:\n",
    "            context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_negative_s,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=42,\n",
    "                name=\"negative_sampling\"\n",
    "            )\n",
    "\n",
    "            # Build context and label vectors (for one center word)\n",
    "            negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "\n",
    "            # Concat negative samples with true context word (positive sample)\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "            # label 1 for positive sample and 0 for negative samples.\n",
    "            label = tf.constant([1] + [0] * num_negative_s, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            centers.append(center_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return centers, contexts, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert word tokens to their id with word_to_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenss = []\n",
    "for row in data[\"tokens\"].dropna().values:\n",
    "    if row:\n",
    "        tokenss.append([word_to_id[token] for token in row])\n",
    "    # print(row)\n",
    "tokenss[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# max_length = 10\n",
    "# padded_tokens = pad_sequences(tokenss, padding='post')\n",
    "# padded_tokens.shape\n",
    "# padded_tokens = padded_tokens.tolist()\n",
    "# padded_tokens = [np.array(lst) for lst in padded_tokens]\n",
    "\n",
    "tokenss = [np.array(lst) for lst in tokenss]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(tokenss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create ((center words, contexts words,), labels) for feed to network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "num_negative_samples = 200\n",
    "vocab_size = len(word_to_id) + 1\n",
    "\n",
    "centers, contexts, labels = generate_training_data(\n",
    "    sentences=tokenss,\n",
    "    window_size=window_size,\n",
    "    num_negative_s=num_negative_samples,\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "centers = np.array(centers)\n",
    "contexts = np.array(contexts)[:, :, 0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {centers.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "centers[:10]\n",
    "print()\n",
    "contexts[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use tensorflow caching feature and set batch size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((centers, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in dataset.take(1):\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build customize model with keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            input_length=1,\n",
    "            name=\"center_embedding\"\n",
    "        )\n",
    "        self.context_embedding = layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            input_length=num_negative_samples + 1,\n",
    "            name=\"context_embedding\"\n",
    "        )\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        print()\n",
    "        print(f\"target = {target}\")\n",
    "        print(f\"context = {context}\")\n",
    "        # target: (batch, dummy)\n",
    "        # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        print(f\"word_emb = {word_emb}\")\n",
    "\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        print(f\"context_emb = {context_emb}\")\n",
    "\n",
    "        # context_emb: (batch, context, embed)\n",
    "        # Einstein summation:\n",
    "        # define element-wise computation: sum(word_emb * context_emb)\n",
    "        # computes the dot product of target and context embeddings from a training pair\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        print(f\"dots = {dots}\")\n",
    "\n",
    "        # dots: (batch, context)\n",
    "        return dots"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "epochs_ = 50\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "word2vec.fit(dataset, epochs=epochs_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word2vec.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('center_embedding').get_weights()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weights.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save weights of center_embedding layer (word embeddings) to numpy array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name = f'weights_nneg{num_negative_samples}_em{embedding_dim}_ep{epochs_}_vocs{vocab_size}_ws{window_size}'\n",
    "np.save(file_name, weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_file_name = f'weights_nneg{num_negative_samples}_em{embedding_dim}_ep{epochs_}_vocs{vocab_size}_ws{window_size}'\n",
    "loaded_weights = np.load(load_file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Find nearest neighbor word with cosine similarity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_matrix = cosine_similarity(weights, weights)\n",
    "print(cosine_matrix)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cosine_matrix.shape\n",
    "\n",
    "\n",
    "def cosine_similarity_word(words, cosine_matrix, n=10):\n",
    "    for word in words:\n",
    "        similars = []\n",
    "        for id in cosine_matrix[word_to_id[word]].argsort()[::-1][0:n]:\n",
    "            similars.append(id_to_word[id])\n",
    "        print(word, '=', similars, '\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_count_list[30:50]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cosine_similarity_word(['شاه', 'یوسف', 'خسرو', 'گل','دجله'], cosine_matrix, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_to_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, j in word_to_id.items():\n",
    "    if 'کم' in i[-2:]:\n",
    "        print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, j in word_to_id.items():\n",
    "    if '_' in i:\n",
    "        print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KUW4xepi9mvN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_training_data(sentences, window_size, num_negative_s, vocab_size):\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    centers, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for vocab_size tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all sentences in corpus\n",
    "    for sequence in tqdm_notebook(sentences, desc='Sentenses', colour=\"MAGENTA\"):\n",
    "\n",
    "        # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_samples, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size=vocab_size,\n",
    "            sampling_table=sampling_table,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0\n",
    "        )\n",
    "\n",
    "        # Iterate over each positive skip-gram pair to produce training examples\n",
    "        # with positive context word and negative samples.\n",
    "        for center_word, context_word in positive_samples:\n",
    "            context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_negative_s,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=42,\n",
    "                name=\"negative_sampling\"\n",
    "            )\n",
    "\n",
    "            # Build context and label vectors (for one center word)\n",
    "            negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "\n",
    "            # Concat negative samples with true context word (positive sample)\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "            # label 1 for positive sample and 0 for negative samples.\n",
    "            label = tf.constant([1] + [0] * num_negative_s, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            centers.append(center_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return centers, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "RRySGgaqnSDA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Convert word tokens to their id with word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0y500_TQ9Wz4",
    "outputId": "4ccfc5e7-73ef-4a9b-e1ac-d312f238416a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[2565, 976, 386, 1424, 24, 1666, 1094, 229],\n",
       " [256, 1234, 452, 3764, 870, 410, 22],\n",
       " [90, 2566, 664, 2012, 977, 345, 1424, 3765],\n",
       " [165, 3766, 229, 1425, 53, 779, 3767, 612, 236, 871],\n",
       " [37, 499, 1667, 978, 326, 2567, 247, 527, 1668, 103, 1095, 2568, 346],\n",
       " [1668, 613, 84, 2013, 665, 50, 9, 3768, 50, 9, 257, 2569],\n",
       " [780, 84, 614, 26, 283, 615, 84, 614, 615, 91, 257, 3769, 1235, 528],\n",
       " [1096, 10, 159, 1669, 529, 723, 1669, 3770, 69, 291, 1669, 3771, 67, 872],\n",
       " [3772, 124, 75, 1, 3773, 527, 1, 292, 3774, 111, 411, 3775, 1097],\n",
       " [171, 32, 3776, 5, 116, 326, 2570, 412, 214, 1098, 64, 724, 284]]"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "tokenss = []\n",
    "for row in data[\"tokens\"].dropna().values:\n",
    "    if row:\n",
    "        tokenss.append([word_to_id[token] for token in row])\n",
    "    # print(row)\n",
    "tokenss[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "ahyUU3JdBP2Z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# max_length = 10\n",
    "# padded_tokens = pad_sequences(tokenss, padding='post')\n",
    "# padded_tokens.shape\n",
    "# padded_tokens = padded_tokens.tolist()\n",
    "# padded_tokens = [np.array(lst) for lst in padded_tokens]\n",
    "\n",
    "tokenss = [np.array(lst) for lst in tokenss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xq49HZOcDnTh",
    "outputId": "c2b60c4b-3914-4dd6-894f-48794277f7cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5316"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "len(tokenss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "WGlNx4l4nSDE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create ((center words, contexts words,), labels) for feed to network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "afd63f39b87f46da9745675251273c45",
      "c9ddda96b9104aba834954ee14e783c7",
      "dea00032a72942448c3ea81d373ee29f",
      "3b20472e23f240308551359547732c3f",
      "b43f912f67014cbca28ee106e34abc6c",
      "ce9a77b5394742cc82bbda2e67892023",
      "ea2edbdc17004f369af35fcfd3aa61ab",
      "3d3212f92d43489dbd2659dd339532ca",
      "3dfaa4073c9d4a9ab2196c860c36f8fb",
      "e59fb63e784a430fa5e3efa4185aa3a6",
      "66436b6dd8eb416ba6a33deccdaee223"
     ]
    },
    "id": "A_8r-fJeFRrq",
    "outputId": "4b8910f2-e98a-4d5d-a1bd-2993ef513cd7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Sentenses:   0%|          | 0/5316 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afd63f39b87f46da9745675251273c45"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "targets.shape: (69457,)\n",
      "contexts.shape: (69457, 201)\n",
      "labels.shape: (69457, 201)\n"
     ]
    }
   ],
   "source": [
    "window_size = 5\n",
    "num_negative_samples = 200\n",
    "vocab_size = len(word_to_id) + 1\n",
    "\n",
    "centers, contexts, labels = generate_training_data(\n",
    "    sentences=tokenss,\n",
    "    window_size=window_size,\n",
    "    num_negative_s=num_negative_samples,\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "centers = np.array(centers)\n",
    "contexts = np.array(contexts)[:, :, 0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {centers.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rStYwGuYFSgz",
    "outputId": "4bfc287e-4c82-4201-cf5f-7b84b81a439e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1424, 1424, 1424, 1424, 1424, 1424, 1424,  452,  452,  452])"
      ]
     },
     "metadata": {},
     "execution_count": 63
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1666,   14, 1273, ..., 2898, 4240,   71],\n",
       "       [2565,  613,   13, ...,  875, 5675, 2930],\n",
       "       [ 976, 2536, 1029, ...,  151, 1438,  563],\n",
       "       ...,\n",
       "       [ 870,    1,  531, ..., 2068,  608, 2819],\n",
       "       [3764, 5778,  531, ...,   29,  332,  120],\n",
       "       [ 256,    7,  626, ...,   15,  237, 1664]])"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "centers[:10]\n",
    "print()\n",
    "contexts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "0HJfGW9onSDF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Use tensorflow caching feature and set batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "KpVCWt0fFSjt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((centers, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2OYODbNFSmf",
    "outputId": "a38c5a84-55db-4862-a2ac-169e3db3e06e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "((<tf.Tensor: shape=(1024,), dtype=int64, numpy=array([  22, 3917, 4230, ..., 4160, 1442, 2860])>, <tf.Tensor: shape=(1024, 201), dtype=int64, numpy=\n",
      "array([[ 131,  271,  406, ..., 1324,   48,   70],\n",
      "       [ 185,  762, 2120, ..., 4952,  175,  228],\n",
      "       [4232,  987,    3, ...,   33,  165, 6089],\n",
      "       ...,\n",
      "       [1758,   43,   20, ...,  266, 3292,  458],\n",
      "       [1542,    1,   12, ..., 2382,  152,  536],\n",
      "       [ 295,    6, 3240, ...,  977,  978,  168]])>), <tf.Tensor: shape=(1024, 201), dtype=int64, numpy=\n",
      "array([[1, 0, 0, ..., 0, 0, 0],\n",
      "       [1, 0, 0, ..., 0, 0, 0],\n",
      "       [1, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 0, 0, ..., 0, 0, 0],\n",
      "       [1, 0, 0, ..., 0, 0, 0],\n",
      "       [1, 0, 0, ..., 0, 0, 0]])>)\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fSXBvjIHnSDG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Build customize model with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "6EQZu0orFSo3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            input_length=1,\n",
    "            name=\"center_embedding\"\n",
    "        )\n",
    "        self.context_embedding = layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            input_length=num_negative_samples + 1,\n",
    "            name=\"context_embedding\"\n",
    "        )\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        print()\n",
    "        print(f\"target = {target}\")\n",
    "        print(f\"context = {context}\")\n",
    "        # target: (batch, dummy)\n",
    "        # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        print(f\"word_emb = {word_emb}\")\n",
    "\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        print(f\"context_emb = {context_emb}\")\n",
    "\n",
    "        # context_emb: (batch, context, embed)\n",
    "        # Einstein summation:\n",
    "        # define element-wise computation: sum(word_emb * context_emb)\n",
    "        # computes the dot product of target and context embeddings from a training pair\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        print(f\"dots = {dots}\")\n",
    "\n",
    "        # dots: (batch, context)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rq8KldZCUzIV",
    "outputId": "b3142ec9-d35f-406c-c521-c43f6053120b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "\n",
      "target = Tensor(\"IteratorGetNext:0\", shape=(1024,), dtype=int64)\n",
      "context = Tensor(\"IteratorGetNext:1\", shape=(1024, 201), dtype=int64)\n",
      "word_emb = Tensor(\"word2_vec_7/center_embedding/embedding_lookup/Identity_1:0\", shape=(1024, 200), dtype=float32)\n",
      "context_emb = Tensor(\"word2_vec_7/context_embedding/embedding_lookup/Identity_1:0\", shape=(1024, 201, 200), dtype=float32)\n",
      "dots = Tensor(\"word2_vec_7/einsum/Einsum:0\", shape=(1024, 201), dtype=float32)\n",
      "\n",
      "target = Tensor(\"IteratorGetNext:0\", shape=(1024,), dtype=int64)\n",
      "context = Tensor(\"IteratorGetNext:1\", shape=(1024, 201), dtype=int64)\n",
      "word_emb = Tensor(\"word2_vec_7/center_embedding/embedding_lookup/Identity_1:0\", shape=(1024, 200), dtype=float32)\n",
      "context_emb = Tensor(\"word2_vec_7/context_embedding/embedding_lookup/Identity_1:0\", shape=(1024, 201, 200), dtype=float32)\n",
      "dots = Tensor(\"word2_vec_7/einsum/Einsum:0\", shape=(1024, 201), dtype=float32)\n",
      "67/67 [==============================] - 11s 152ms/step - loss: 5.3026 - accuracy: 0.0106\n",
      "Epoch 2/50\n",
      "67/67 [==============================] - 10s 151ms/step - loss: 5.2722 - accuracy: 0.2294\n",
      "Epoch 3/50\n",
      "67/67 [==============================] - 10s 147ms/step - loss: 5.2200 - accuracy: 0.3661\n",
      "Epoch 4/50\n",
      "67/67 [==============================] - 10s 147ms/step - loss: 5.1156 - accuracy: 0.3458\n",
      "Epoch 5/50\n",
      "67/67 [==============================] - 10s 148ms/step - loss: 4.9287 - accuracy: 0.3244\n",
      "Epoch 6/50\n",
      "67/67 [==============================] - 10s 148ms/step - loss: 4.6454 - accuracy: 0.3159\n",
      "Epoch 7/50\n",
      "67/67 [==============================] - 10s 147ms/step - loss: 4.2808 - accuracy: 0.3184\n",
      "Epoch 8/50\n",
      "67/67 [==============================] - 10s 148ms/step - loss: 3.8630 - accuracy: 0.3291\n",
      "Epoch 9/50\n",
      "67/67 [==============================] - 10s 146ms/step - loss: 3.4240 - accuracy: 0.3483\n",
      "Epoch 10/50\n",
      "67/67 [==============================] - 10s 146ms/step - loss: 3.0006 - accuracy: 0.3776\n",
      "Epoch 11/50\n",
      "67/67 [==============================] - 10s 146ms/step - loss: 2.6214 - accuracy: 0.4164\n",
      "Epoch 12/50\n",
      "67/67 [==============================] - 10s 144ms/step - loss: 2.2962 - accuracy: 0.4621\n",
      "Epoch 13/50\n",
      "67/67 [==============================] - 10s 143ms/step - loss: 2.0231 - accuracy: 0.5105\n",
      "Epoch 14/50\n",
      "67/67 [==============================] - 10s 143ms/step - loss: 1.7963 - accuracy: 0.5588\n",
      "Epoch 15/50\n",
      "67/67 [==============================] - 10s 143ms/step - loss: 1.6095 - accuracy: 0.6000\n",
      "Epoch 16/50\n",
      "67/67 [==============================] - 10s 143ms/step - loss: 1.4565 - accuracy: 0.6358\n",
      "Epoch 17/50\n",
      "67/67 [==============================] - 10s 144ms/step - loss: 1.3313 - accuracy: 0.6665\n",
      "Epoch 18/50\n",
      "67/67 [==============================] - 10s 144ms/step - loss: 1.2289 - accuracy: 0.6905\n",
      "Epoch 19/50\n",
      "67/67 [==============================] - 10s 145ms/step - loss: 1.1449 - accuracy: 0.7099\n",
      "Epoch 20/50\n",
      "67/67 [==============================] - 10s 145ms/step - loss: 1.0755 - accuracy: 0.7246\n",
      "Epoch 21/50\n",
      "67/67 [==============================] - 10s 145ms/step - loss: 1.0179 - accuracy: 0.7376\n",
      "Epoch 22/50\n",
      "67/67 [==============================] - 10s 145ms/step - loss: 0.9698 - accuracy: 0.7470\n",
      "Epoch 23/50\n",
      "67/67 [==============================] - 10s 145ms/step - loss: 0.9292 - accuracy: 0.7550\n",
      "Epoch 24/50\n",
      "67/67 [==============================] - 10s 145ms/step - loss: 0.8948 - accuracy: 0.7611\n",
      "Epoch 25/50\n",
      "67/67 [==============================] - 10s 144ms/step - loss: 0.8654 - accuracy: 0.7655\n",
      "Epoch 26/50\n",
      "67/67 [==============================] - 10s 146ms/step - loss: 0.8401 - accuracy: 0.7689\n",
      "Epoch 27/50\n",
      "67/67 [==============================] - 10s 146ms/step - loss: 0.8182 - accuracy: 0.7715\n",
      "Epoch 28/50\n",
      "67/67 [==============================] - 10s 146ms/step - loss: 0.7992 - accuracy: 0.7738\n",
      "Epoch 29/50\n",
      "67/67 [==============================] - 10s 145ms/step - loss: 0.7825 - accuracy: 0.7752\n",
      "Epoch 30/50\n",
      "67/67 [==============================] - 10s 146ms/step - loss: 0.7678 - accuracy: 0.7764\n",
      "Epoch 31/50\n",
      "67/67 [==============================] - 10s 146ms/step - loss: 0.7549 - accuracy: 0.7775\n",
      "Epoch 32/50\n",
      "67/67 [==============================] - 10s 150ms/step - loss: 0.7433 - accuracy: 0.7782\n",
      "Epoch 33/50\n",
      "67/67 [==============================] - 10s 146ms/step - loss: 0.7330 - accuracy: 0.7786\n",
      "Epoch 34/50\n",
      "67/67 [==============================] - 10s 147ms/step - loss: 0.7238 - accuracy: 0.7788\n",
      "Epoch 35/50\n",
      "67/67 [==============================] - 10s 147ms/step - loss: 0.7155 - accuracy: 0.7791\n",
      "Epoch 36/50\n",
      "67/67 [==============================] - 10s 145ms/step - loss: 0.7079 - accuracy: 0.7792\n",
      "Epoch 37/50\n",
      "67/67 [==============================] - 10s 146ms/step - loss: 0.7011 - accuracy: 0.7793\n",
      "Epoch 38/50\n",
      "67/67 [==============================] - 10s 147ms/step - loss: 0.6949 - accuracy: 0.7794\n",
      "Epoch 39/50\n",
      "67/67 [==============================] - 10s 147ms/step - loss: 0.6892 - accuracy: 0.7796\n",
      "Epoch 40/50\n",
      "67/67 [==============================] - 10s 147ms/step - loss: 0.6841 - accuracy: 0.7797\n",
      "Epoch 41/50\n",
      "67/67 [==============================] - 10s 145ms/step - loss: 0.6793 - accuracy: 0.7799\n",
      "Epoch 42/50\n",
      "67/67 [==============================] - 10s 145ms/step - loss: 0.6749 - accuracy: 0.7801\n",
      "Epoch 43/50\n",
      "67/67 [==============================] - 10s 145ms/step - loss: 0.6708 - accuracy: 0.7802\n",
      "Epoch 44/50\n",
      "67/67 [==============================] - 10s 147ms/step - loss: 0.6671 - accuracy: 0.7801\n",
      "Epoch 45/50\n",
      "67/67 [==============================] - 10s 145ms/step - loss: 0.6636 - accuracy: 0.7801\n",
      "Epoch 46/50\n",
      "67/67 [==============================] - 10s 144ms/step - loss: 0.6603 - accuracy: 0.7801\n",
      "Epoch 47/50\n",
      "67/67 [==============================] - 10s 144ms/step - loss: 0.6573 - accuracy: 0.7800\n",
      "Epoch 48/50\n",
      "67/67 [==============================] - 10s 145ms/step - loss: 0.6545 - accuracy: 0.7799\n",
      "Epoch 49/50\n",
      "67/67 [==============================] - 10s 146ms/step - loss: 0.6519 - accuracy: 0.7799\n",
      "Epoch 50/50\n",
      "67/67 [==============================] - 10s 146ms/step - loss: 0.6494 - accuracy: 0.7798\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f59a3e4a690>"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "embedding_dim = 200\n",
    "epochs_ = 50\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "word2vec.fit(dataset, epochs=epochs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Iuga96he-sI8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ff6d10a2-38a2-4da2-b9e7-9b142d83428c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"word2_vec_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " center_embedding (Embedding  multiple                 1784400   \n",
      " )                                                               \n",
      "                                                                 \n",
      " context_embedding (Embeddin  multiple                 1784400   \n",
      " g)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,568,800\n",
      "Trainable params: 3,568,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word2vec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "3IjCRWT2UzMp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('center_embedding').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "nzOFekvTUzQE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5bd3d080-2f11-4b24-f3e8-b29e4a24417e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8922, 200)"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ux_EmxI9nSDI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Save weights of center_embedding layer (word embeddings) to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "nhqiH-qBmLHg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_name = f'weights_nneg{num_negative_samples}_em{embedding_dim}_ep{epochs_}_vocs{vocab_size}_ws{window_size}'\n",
    "np.save(file_name, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrlv5pVQXRf7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "load_file_name = f'weights_nneg{num_negative_samples}_em{embedding_dim}_ep{epochs_}_vocs{vocab_size}_ws{window_size}'\n",
    "loaded_weights = np.load(load_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "x3wJq0P4nSDJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Find nearest neighbor word with cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRcLLGhGGmfe",
    "outputId": "60e5f6fe-f400-45b9-9d7a-8d3e44d41241",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1.0000001  -0.0833027  -0.02161321 ...  0.03165701  0.00011451\n",
      "  -0.06165543]\n",
      " [-0.0833027   1.          0.08032579 ...  0.01039312 -0.05719611\n",
      "  -0.01495304]\n",
      " [-0.02161321  0.08032579  1.         ... -0.0136863   0.07858521\n",
      "   0.01053756]\n",
      " ...\n",
      " [ 0.03165701  0.01039312 -0.0136863  ...  0.9999998  -0.19599128\n",
      "   0.08485357]\n",
      " [ 0.00011451 -0.05719611  0.07858521 ... -0.19599128  0.99999994\n",
      "  -0.08715412]\n",
      " [-0.06165543 -0.01495304  0.01053756 ...  0.08485357 -0.08715412\n",
      "   0.9999998 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_matrix = cosine_similarity(weights, weights)\n",
    "print(cosine_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCTzJRR_ueRJ",
    "outputId": "3c5aba96-715f-4871-ac0e-4054cef54daf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8922, 8922)"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "cosine_matrix.shape\n",
    "\n",
    "\n",
    "def cosine_similarity_word(words, cosine_matrix, n=10):\n",
    "    for word in words:\n",
    "        similars = []\n",
    "        for id in cosine_matrix[word_to_id[word]].argsort()[::-1][0:n]:\n",
    "            similars.append(id_to_word[id])\n",
    "        print(word, '=', similars, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DNDHBiDhFac2",
    "outputId": "0ef23380-faa1-4d5f-8459-aa09932c481d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('مه', 114),\n",
       " ('بس', 111),\n",
       " ('دانست#دان', 111),\n",
       " ('ماه', 110),\n",
       " ('کان', 109),\n",
       " ('پر', 109),\n",
       " ('روح', 107),\n",
       " ('عالم', 106),\n",
       " ('نی', 103),\n",
       " ('ره', 102),\n",
       " ('باد', 100),\n",
       " ('تن', 100),\n",
       " ('تبریز', 100),\n",
       " ('کار', 98),\n",
       " ('آنک', 98),\n",
       " ('گرفت#گیر', 95),\n",
       " ('خون', 93),\n",
       " ('پا', 92),\n",
       " ('رخ', 92),\n",
       " ('گه', 91)]"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "sorted_count_list[30:50]"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "M2MqoVMGIloh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_mtFn5jQnSDL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "uspZBpGlHXa8",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngg5QhPQXhxD",
    "outputId": "9b4c3414-edb1-4605-ad24-6b5b54e347dd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "شاه = ['شاه', 'فتادست', 'فساق', 'همامست', 'مباش', 'خسروان', 'شهرست', 'بندگان', 'مکافات', 'گردد\\u200cگر'] \n",
      "\n",
      "یوسف = ['یوسف', 'سیماست', 'زلیخا', 'بریدند', 'نتیجه', 'اندرنگر', 'تمامست', 'دهل', 'اعلا', 'بیچاره'] \n",
      "\n",
      "خسرو = ['خسرو', 'ما\\u200cگر', 'خورشیدروی', 'قباد', 'بشاید', 'بازبیاریم', 'مخدوم', 'انس', 'سرور', 'حسرت'] \n",
      "\n",
      "گل = ['گل', 'هامونست', 'بدریده\\u200cای', 'میوه', 'گرینده', 'سامریست', 'اندرفتد', 'بخندان', 'مپیچان', 'گوارد'] \n",
      "\n",
      "دجله = ['دجله', 'فرات', 'جیحون', 'مقیم', 'صخره', 'صما', 'پرنم', 'بدندی', 'زهر', 'ویس'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity_word(['شاه', 'یوسف', 'خسرو', 'گل','دجله'], cosine_matrix, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6zZyXS0FCa0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aEu-adE2UzzP",
    "outputId": "98571d76-a47d-4572-ce44-081d937a17e9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "کم\n",
      "حکم\n",
      "شکم\n",
      "حاکم\n",
      "هواکم\n",
      "فقدکم\n",
      "اصحابکم\n",
      "اعقابکم\n",
      "جنبکم\n",
      "ذنبکم\n",
      "ربکم\n",
      "تفاحکم\n",
      "اصباحکم\n",
      "ارواحکم\n",
      "اریاحکم\n",
      "یعقوبکم\n",
      "قدامکم\n",
      "دونکم\n",
      "لحظکم\n",
      "لقیاکم\n",
      "لقائکم\n",
      "شدکم\n",
      "بهواکم\n",
      "عنکم\n",
      "فناکم\n",
      "رایناکم\n",
      "بضیاکم\n",
      "بلاکم\n",
      "غیرکم\n",
      "سواکم\n",
      "حورکم\n",
      "احیاکم\n",
      "حیاتکم\n",
      "یترککم\n",
      "ودکم\n",
      "خلاکم\n",
      "فدیتکم\n",
      "قتیلکم\n",
      "فاتکم\n",
      "بدتکم\n",
      "ایبکم\n",
      "می‌کم\n"
     ]
    }
   ],
   "source": [
    "for i, j in word_to_id.items():\n",
    "    if 'کم' in i[-2:]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPT7884wdz_f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i, j in word_to_id.items():\n",
    "    if '_' in i:\n",
    "        print(i)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "shQEzqlj-HZX",
    "KmF8QUe3nSC6",
    "tt6qeckAnSC8"
   ],
   "name": "word2vec_negative_sampling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "afd63f39b87f46da9745675251273c45": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c9ddda96b9104aba834954ee14e783c7",
       "IPY_MODEL_dea00032a72942448c3ea81d373ee29f",
       "IPY_MODEL_3b20472e23f240308551359547732c3f"
      ],
      "layout": "IPY_MODEL_b43f912f67014cbca28ee106e34abc6c"
     }
    },
    "c9ddda96b9104aba834954ee14e783c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce9a77b5394742cc82bbda2e67892023",
      "placeholder": "​",
      "style": "IPY_MODEL_ea2edbdc17004f369af35fcfd3aa61ab",
      "value": "Sentenses: 100%"
     }
    },
    "dea00032a72942448c3ea81d373ee29f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d3212f92d43489dbd2659dd339532ca",
      "max": 5316,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3dfaa4073c9d4a9ab2196c860c36f8fb",
      "value": 5316
     }
    },
    "3b20472e23f240308551359547732c3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e59fb63e784a430fa5e3efa4185aa3a6",
      "placeholder": "​",
      "style": "IPY_MODEL_66436b6dd8eb416ba6a33deccdaee223",
      "value": " 5316/5316 [00:17&lt;00:00, 431.85it/s]"
     }
    },
    "b43f912f67014cbca28ee106e34abc6c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce9a77b5394742cc82bbda2e67892023": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea2edbdc17004f369af35fcfd3aa61ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d3212f92d43489dbd2659dd339532ca": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3dfaa4073c9d4a9ab2196c860c36f8fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": "MAGENTA",
      "description_width": ""
     }
    },
    "e59fb63e784a430fa5e3efa4185aa3a6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66436b6dd8eb416ba6a33deccdaee223": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}